# -*- coding: utf-8 -*-
"""BS6200-Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1swC4dwbcUBZrMySzGxiJZTtC7dyCPAXy

# Download the data
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

# download the data
df_train = pd.read_csv('/content/drive/MyDrive/BS6200/dengue_features_train.csv')
df_train

df_label = pd.read_csv('/content/drive/MyDrive/BS6200/dengue_labels_train.csv')
df_label

# convert the "week_start_date" data into time series data
df_train['total_cases'] = df_label['total_cases']
df_train['week_start_date']=pd.to_datetime(df_train['week_start_date'])
df_train.set_index(df_train['week_start_date'], inplace=True)

df_sj = df_train[df_train['city']=='sj']
df_sj = df_sj.drop(['city'],axis = 1)
df_sj

df_iq = df_train[df_train['city']=='iq']
df_iq = df_iq.drop(['city'],axis = 1)
df_iq

"""# Exploratory data analysis"""

# plot the total cases as time seise and do comparation
f, ax = plt.subplots(figsize=(14, 9))
sns.lineplot(x="week_start_date", y="total_cases", hue ='city', data=df_train)

f, ax = plt.subplots(figsize=(10, 9))
sns.countplot(data = df_train, x = 'city')

# plot the histogram of cities
f, ax = plt.subplots(figsize=(10, 6))
sns.histplot(data=df_train, x="total_cases", hue ='city',kde=True)

f, ax = plt.subplots(figsize=(8, 5))
sns.violinplot(x = 'city',y="total_cases",data=df_train)

# compare the trend of features 
plt.figure(figsize=(18,9))
plt.subplot(1,2,1)
df_sj['ndvi_ne'].plot()
df_sj['ndvi_nw'].plot()
df_sj['ndvi_se'].plot()
df_sj['ndvi_sw'].plot()
plt.xlabel('San Juan ndvi')
plt.legend()
plt.subplot(1,2,2)
df_iq['ndvi_ne'].plot()
df_iq['ndvi_nw'].plot()
df_iq['ndvi_se'].plot()
df_iq['ndvi_sw'].plot()
plt.xlabel('Iquitos Variable ndvi')
plt.legend()

plt.figure(figsize=(18,9))
plt.subplot(1,2,1)
df_sj['reanalysis_avg_temp_k'].plot()
df_sj['reanalysis_max_air_temp_k'].plot()
df_sj['reanalysis_min_air_temp_k'].plot()
plt.xlabel('San Juan temp')
plt.legend()
plt.subplot(1,2,2)
df_iq['reanalysis_avg_temp_k'].plot()
df_iq['reanalysis_max_air_temp_k'].plot()
df_iq['reanalysis_min_air_temp_k'].plot()
plt.xlabel('Iquitos temp')
plt.legend()

plt.figure(figsize=(18,9))
plt.subplot(1,2,1)
df_sj['reanalysis_sat_precip_amt_mm'].plot()
df_sj['reanalysis_precip_amt_kg_per_m2'].plot()
df_sj['station_precip_mm'].plot()
plt.xlabel('San Juan ndvi')
plt.legend()
plt.subplot(1,2,2)
df_iq['reanalysis_sat_precip_amt_mm'].plot()
df_iq['reanalysis_precip_amt_kg_per_m2'].plot()
df_iq['station_precip_mm'].plot()
plt.xlabel('Iquitos Variable ndvi')
plt.legend()

f, ax = plt.subplots(figsize=(14, 9))
sns.scatterplot(x = 'weekofyear',y = 'total_cases',data = df_train, hue='city')

sns.lmplot(x='total_cases', y='weekofyear', data=df_train, markers='o', col='city', hue='city',aspect=1.5, x_jitter=.1)

# plot the correlation of features with total cases
plt.figure(figsize=(14,9))
g=sns.lmplot(x='total_cases', y='ndvi_ne', data=df_train, markers='o', col='city', hue='city',aspect=1.5, x_jitter=.1)
g=sns.lmplot(x='total_cases', y='ndvi_nw', data=df_train, markers='v', col='city', hue='city',aspect=1.5, x_jitter=.1)
g=sns.lmplot(x='total_cases', y='ndvi_se', data=df_train, markers='s', col='city', hue='city',aspect=1.5, x_jitter=.1)
g=sns.lmplot(x='total_cases', y='ndvi_sw', data=df_train, markers='^', col='city', hue='city',aspect=1.5, x_jitter=.1)

g=sns.lmplot(x='total_cases', y='reanalysis_air_temp_k', data=df_train, markers='o', col='city', hue='city',aspect=1.5, x_jitter=.1)
g=sns.lmplot(x='total_cases', y='reanalysis_avg_temp_k', data=df_train, markers='v', col='city', hue='city',aspect=1.5, x_jitter=.1)
g=sns.lmplot(x='total_cases', y='reanalysis_dew_point_temp_k', data=df_train, markers='s', col='city', hue='city',aspect=1.5, x_jitter=.1)
g=sns.lmplot(x='total_cases', y='reanalysis_max_air_temp_k', data=df_train, markers='^', col='city', hue='city',aspect=1.5, x_jitter=.1)
g=sns.lmplot(x='total_cases', y='reanalysis_min_air_temp_k', data=df_train, markers='^', col='city', hue='city',aspect=1.5, x_jitter=.1)

g=sns.lmplot(x='total_cases', y='precipitation_amt_mm', data=df_train, markers='o', col='city', hue='city',aspect=1.5, x_jitter=.1)
g=sns.lmplot(x='total_cases', y='reanalysis_sat_precip_amt_mm', data=df_train, markers='v', col='city', hue='city',aspect=1.5, x_jitter=.1)
g=sns.lmplot(x='total_cases', y='reanalysis_precip_amt_kg_per_m2', data=df_train, markers='s', col='city', hue='city',aspect=1.5, x_jitter=.1)
g=sns.lmplot(x='total_cases', y='station_precip_mm', data=df_train, markers='^', col='city', hue='city',aspect=1.5, x_jitter=.1)

g=sns.lmplot(x='total_cases', y='reanalysis_relative_humidity_percent', data=df_train, markers='o', col='city', hue='city',aspect=1.5, x_jitter=.1)
g=sns.lmplot(x='total_cases', y='reanalysis_specific_humidity_g_per_kg', data=df_train, markers='v', col='city', hue='city',aspect=1.5, x_jitter=.1)

"""# Clean the Data"""

##check whether there are missing values
df_sj.isnull().sum()

# check the histogram of ndvi_ne
sns.histplot(data = df_sj, x = "ndvi_ne",kde=True )

df_sj['ndvi_ne'].fillna(df_sj['ndvi_ne'].mean(),inplace=True)
df_sj['ndvi_ne'].isnull().any()

# fill null values with mean values
df_sj.fillna(df_sj.mean(), inplace=True)
df_sj.isnull().any()

df_iq.isnull().sum()

df_iq.fillna(df_sj.mean(), inplace=True)
df_iq.isnull().any()

"""# Feature Selection

For feature analysis, the data should not involve time series values. So we need to drop the 'week_start_date' column.
"""

# we hope the variables are as indepent as possiable. We need to drop the similar features, corr  > 0.95
sj_features = df_sj.drop(['year','weekofyear'],axis = 1)
iq_features = df_iq.drop(['year','weekofyear'],axis = 1)
sj_corr = sj_features.corr()
iq_corr = iq_features.corr()
sj_features.shape

# plot san juan
plt.figure(figsize = (16,9))
sj_corr_heat = sns.heatmap(sj_corr,annot=True,xticklabels=90)
plt.title('San Juan Variable Correlations')

# plot iquitos
plt.figure(figsize = (16,9))
iq_corr_heat = sns.heatmap(iq_corr,annot=True)
plt.title('Iquitos Variable Correlations')

# drop the similar columns
iq_features.drop(['reanalysis_sat_precip_amt_mm','reanalysis_dew_point_temp_k','reanalysis_air_temp_k'],axis = 1,inplace=True)
sj_features.drop(['reanalysis_sat_precip_amt_mm','reanalysis_dew_point_temp_k','reanalysis_air_temp_k'],axis = 1,inplace=True)
iq_features.shape

""" we can see that there are few samples with multiple missing values. we should just rule out those only contain 4 or 8 vaild values."""

sj_corr = sj_features.corr()
iq_corr = iq_features.corr()

# plot san juan
plt.figure(figsize = (16,9))
sj_corr_heat = sns.heatmap(sj_corr,annot=True)
plt.title('San Juan Variable Correlations')

# plot iquitos
plt.figure(figsize = (16,9))
iq_corr_heat = sns.heatmap(iq_corr,annot=True)
plt.title('Iquitos Variable Correlations')

"""# Feature Aggregation"""

sj_features.columns

# According to the heatmap of those two cities, we should combine the ndvi and the reanerlize temp and station temp. 
ndvi_avg = sj_features.iloc[:,1:5].mean(1) # use the mean value of four ndvi to replease original ndvi
sj_agg = sj_features.drop(['ndvi_ne','ndvi_nw','ndvi_se','ndvi_sw','reanalysis_max_air_temp_k','reanalysis_min_air_temp_k','station_max_temp_c','station_min_temp_c', 'station_avg_temp_c','reanalysis_specific_humidity_g_per_kg'],axis = 1)
sj_agg['ndvi_avg'] = ndvi_avg
sj_agg

sj_agg_corr = sj_agg.corr()
plt.figure(figsize = (16,9))
sj_corr_heat = sns.heatmap(sj_agg_corr,annot=True)
plt.title('San Juan Variable Correlations')

"""Still need to do feature selection. We need to plot the result of iq to choose select which one"""

# According to the heatmap of those two cities, we should combine the ndvi and the reanerlize temp and station temp. 
iq_ndvi_avg = iq_features.iloc[:,1:5].mean(1) # use the mean value of four ndvi to replease original ndvi
iq_agg = iq_features.drop(['ndvi_ne','ndvi_nw','ndvi_se','ndvi_sw','reanalysis_max_air_temp_k','reanalysis_min_air_temp_k', 'station_max_temp_c','station_min_temp_c','station_avg_temp_c','reanalysis_specific_humidity_g_per_kg'],axis = 1)
iq_agg['ndvi_avg'] = iq_ndvi_avg
iq_agg

iq_agg_corr = iq_agg.corr()
plt.figure(figsize = (16,9))
iq_corr_heat = sns.heatmap(iq_agg_corr,annot=True)
plt.title('Iquitos Variable Correlations')

# San Juan
plt.figure(figsize=(20,9))
plt.subplot(1,2,1)
(sj_agg_corr
     .total_cases
     .drop('total_cases') # don't compare with myself
     .sort_values(ascending=True)
     .plot
     .barh())
plt.subplot(1,2,2)
(iq_agg_corr
     .total_cases
     .drop('total_cases') # don't compare with myself
     .sort_values(ascending=True)
     .plot
     .barh())

"""# Split and Normalization"""

sj_pick.describe()

plt.figure(figsize=(20,9))
plt.subplot(1,3,1)
sns.violinplot(x = 'city',y="reanalysis_avg_temp_k",data=df_train)
plt.subplot(1,3,2)
sns.violinplot(x = 'city',y="precipitation_amt_mm",data=df_train)
plt.subplot(1,3,3)
sns.violinplot(x = 'city',y="reanalysis_relative_humidity_percent",data=df_train)

"""use z-sore normalization"""

plt.figure(figsize=(20,9))
plt.subplot(1,3,1)
sns.boxplot(x = 'city',y="reanalysis_avg_temp_k",data=df_train)
plt.subplot(1,3,2)
sns.boxplot(x = 'city',y="precipitation_amt_mm",data=df_train)
plt.subplot(1,3,3)
sns.boxplot(x = 'city',y="reanalysis_relative_humidity_percent",data=df_train)

plt.figure(figsize=(20,9))
plt.subplot(1,3,1)
sns.histplot(x="reanalysis_avg_temp_k",hue = 'city',data=df_train)
plt.subplot(1,3,2)
sns.histplot(x="reanalysis_relative_humidity_percent",hue = 'city',data=df_train)
# plt.subplot(1,3,3)
# sns.histplot(x = 'city',y="reanalysis_relative_humidity_percent",data=df_train)

# set the y label
y_sj = df_label[df_label['city']=='sj']['total_cases']
y_iq = df_label[df_label['city']=='iq']['total_cases']

df_sj

# Normalization
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
xnor_sj = sc.fit_transform(sj_agg) 


xnor_iq = sc.fit_transform(iq_agg)

# Split dataset X nn train and test set
from sklearn.model_selection import TimeSeriesSplit
tscv = TimeSeriesSplit(n_splits=3) # test size account for 0.2
print(tscv)

for train_index, test_index in tscv.split(sj_pick):
  X_train_sj, X_test_sj = sj_pick.iloc[train_index, :], sj_pick.iloc[test_index,:]
  y_train_sj, y_test_sj = y_sj.iloc[train_index], y_sj.iloc[test_index]

for train_index, test_index in tscv.split(iq_pick):
  X_train_iq, X_test_iq = iq_pick.iloc[train_index, :], iq_pick.iloc[test_index,:]
  y_train_iq, y_test_iq = y_iq.iloc[train_index], y_iq.iloc[test_index]

for train_index, test_index in tscv.split(xnor_sj):
  X_train_sj, X_test_sj = xnor_sj[train_index, :], xnor_sj[test_index,:]
  y_train_sj, y_test_sj = y_sj.iloc[train_index], y_sj.iloc[test_index]

for train_index, test_index in tscv.split(xnor_iq):
  X_train_iq, X_test_iq = xnor_iq[train_index, :], xnor_iq[test_index,:]
  y_train_iq, y_test_iq = y_iq.iloc[train_index], y_iq.iloc[test_index]

X_train_iq  #check the result of spliting

X_test_iq

# Normalization
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
xtrain_sj = sc.fit_transform(X_train_sj) 
xtest_sj = sc.fit_transform(X_test_sj) 

xtrain_iq = sc.fit_transform(X_train_iq) 
xtest_iq = sc.fit_transform(X_test_iq)

xtrain_iq.shape



"""# PCA"""

df_sj

df_iq



df_sj = df_sj.drop(['week_start_date'],axis=1)
df_iq = df_iq.drop(['week_start_date'],axis=1)



df_iq_t

# Normalization
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
Xnor_sj = sc.fit_transform(df_sj) 

Xnor_iq = sc.fit_transform(df_iq)

Xnor_sj

from sklearn.decomposition import PCA
"""Process data with PCA, PC25 is 90% of original data"""
pc = 0
pca = PCA(n_components=20)
pca.fit(Xnor_sj)
sj_pca = pca.fit_transform(Xnor_sj)
ratio = pca.explained_variance_ratio_
for i in range(11):
  pc += ratio[i]
print(pc)
"""plot the bar chart of PCA"""
plt.bar(range(20),ratio)
plt.xlabel("PC")
plt.ylabel("The ratio of PC")
plt.show()

from sklearn.decomposition import PCA
pca = PCA(n_components=11)
pca.fit(Xnor_sj)
sj_pca = pca.fit_transform(Xnor_sj)
sj_pca

from sklearn.decomposition import PCA
"""Process data with PCA, PC25 is 90% of original data"""
pc = 0
pca = PCA(n_components=20)
pca.fit(Xnor_sj)
iq_pca = pca.fit_transform(Xnor_iq)
ratio = pca.explained_variance_ratio_
for i in range(12):
  pc += ratio[i]
print(pc)
"""plot the bar chart of PCA"""
plt.bar(range(20),ratio)
plt.xlabel("PC")
plt.ylabel("The ratio of PC")
plt.show()

pca = PCA(n_components=12)
pca.fit(Xnor_iq)
iq_pca = pca.fit_transform(Xnor_iq)

from sklearn.model_selection import TimeSeriesSplit
tscv = TimeSeriesSplit(n_splits=3) # test size account for 0.2
for train_index, test_index in tscv.split(sj_pca):
  Xtrain_sj, Xtest_sj = sj_pca[train_index, :], sj_pca[test_index,:]
  y_train_sj, y_test_sj = y_sj.iloc[train_index], y_sj.iloc[test_index]

for train_index, test_index in tscv.split(iq_pca):
  Xtrain_iq, Xtest_iq = iq_pca[train_index, :], iq_pca[test_index,:]
  y_train_iq, y_test_iq = y_iq.iloc[train_index], y_iq.iloc[test_index]

from sklearn.model_selection import TimeSeriesSplit
tscv = TimeSeriesSplit(n_splits=3) # test size account for 0.2
for train_index, test_index in tscv.split(Xnor_sj):
  Xtrain_sj, Xtest_sj = sj_pca[train_index, :], sj_pca[test_index,:]
  y_train_sj, y_test_sj = y_sj.iloc[train_index], y_sj.iloc[test_index]

for train_index, test_index in tscv.split(Xnor_iq):
  Xtrain_iq, Xtest_iq = iq_pca[train_index, :], iq_pca[test_index,:]
  y_train_iq, y_test_iq = y_iq.iloc[train_index], y_iq.iloc[test_index]

print(len(Xtest_sj))
print(len(y_test_sj))

"""# Linear Regression"""

from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score

# Linear Regression
LR_sj = LinearRegression()
LR_sj_fit = LR_sj.fit(Xtrain_sj, y_train_sj)
LR_sj_pred=LR_sj_fit.predict(Xtest_sj)
r2_LR_sj = r2_score(y_test_sj,LR_sj_pred)
MSE = mean_squared_error(y_test_sj, LR_sj_pred)
MAE_l = mean_absolute_error(y_test_sj, LR_sj_pred)
print("MAE :", MAE_l)  
print("MSE :", MSE)
print("R2_Score :",r2_LR_sj)

year_lis = list(df_sj['year']) + list(df_sj['year'][702:936])
cases_lis = list(y_sj) + list(LR_sj_pred)
label = ['Original']* 935 + ['Predicted'] * 235
df_plot = pd.DataFrame({'year':year_lis,'total_cases':cases_lis,'label':label})
df_plot

# visualization
plt.figure(figsize=(16,9))
sns.relplot(data = df_plot,x = 'year',y = 'total_cases',hue  = 'label',kind="line")
plt.title('LinearRegression Result of San Juan')

# Linear Regression
LR_iq = LinearRegression()
LR_iq_fit = LR_sj.fit(Xtrain_iq, y_train_iq)
LR_iq_pred=LR_sj_fit.predict(Xtest_iq)
r2_LR_iq = r2_score(y_test_iq,LR_iq_pred)
MSE = mean_squared_error(y_test_iq,LR_iq_pred)
MAE = mean_absolute_error(y_test_iq, LR_iq_pred)
print("MAE :", MAE) 
print("MSE :", MSE)
print("R2_Score :",r2_LR_iq)

df_iq['year']

year_lis = list(df_iq['year']) + list(df_iq['year'][390:521])
cases_lis = list(y_iq) + list(LR_iq_pred)
label = ['Original']* 520 + ['Predicted'] * 130
df_plot_iq = pd.DataFrame({'year':year_lis,'total_cases':cases_lis,'label':label})

plt.figure(figsize=(16,9))
sns.relplot(data = df_plot_iq,x = 'year',y = 'total_cases',hue  = 'label',kind="line")
plt.title('LinearRegression Result of Iquitos')



"""# Random Forest Regression"""

MAE_LIS = []
for i in range(10,300,10):
  rf = RandomForestRegressor(n_estimators = i)
  rf_fit = rf.fit(Xtrain_sj, y_train_sj)
  rf_pred = rf_fit.predict(Xtest_sj)
  MAE_l = mean_absolute_error(y_test_sj, rf_pred)
  MAE_LIS.append(MAE_l)
plt.plot(range(10,300,10),MAE_LIS)
plt.show()

rf = RandomForestRegressor(n_estimators = 175)
rf_fit = rf.fit(Xtrain_sj, y_train_sj)
rf_pred = rf_fit.predict(Xtest_sj)
r2_rf_sj = r2_score(y_test_sj,rf_pred)
MSE = mean_squared_error(y_test_sj, rf_pred)
MAE_l = mean_absolute_error(y_test_sj, rf_pred)
print("MAE :", MAE_l)  
print("MSE :", MSE)
print("R2_Score :",r2_LR_sj)

rf_cases = list(y_sj) + list(rf_pred)
df_plot['rf_pred'] = rf_cases
plt.figure(figsize=(16,9))
sns.relplot(data = df_plot,x = 'year',y = 'rf_pred',hue  = 'label',kind="line")
plt.title('Random Forest Regression Result of San Juan')

rf = RandomForestRegressor(n_estimators = 100)
rf_fit = rf.fit(Xtrain_iq, y_train_iq)
rf_pred = rf_fit.predict(Xtest_iq)
r2_rf = r2_score(y_test_iq,rf_pred)
MSE = mean_squared_error(y_test_iq, rf_pred)
MAE_l = mean_absolute_error(y_test_iq, rf_pred)
print("MAE :", MAE_l)  
print("MSE :", MSE)
print("R2_Score :",r2_rf)

rf_cases = list(y_iq) + list(rf_pred)
df_plot_iq['rf_pred'] = rf_cases
plt.figure(figsize=(16,9))
sns.relplot(data = df_plot_iq,x = 'year',y = 'rf_pred',hue  = 'label',kind="line")
plt.title('Random Forest Regression Result of Iquitos')

"""# Support Vector Regressor"""

from sklearn.svm import SVR
regressor = SVR(kernel = 'rbf')
regressor.fit(xtrain_sj, y_train_sj)
SVR_pred = regressor.predict(xtest_sj)
r2_SVR = r2_score(y_test_sj,SVR_pred)
MSE = mean_squared_error(y_test_sj, SVR_pred)
MAE_l = mean_absolute_error(y_test_sj, SVR_pred)
print("MAE :", MAE_l)  
print("MSE :", MSE)
print("R2_Score :",r2_SVR)

year_lis = list(df_sj['year']) + list(df_sj['year'][702:936])
cases_lis = list(y_sj) + list(SVR_pred)
label = ['Original']* 935 + ['Predicted'] * 235
df_plot = pd.DataFrame({'year':year_lis,'total_cases':cases_lis,'label':label})
df_plot

SVR_cases = list(y_sj) + list(SVR_pred)
df_plot['SVR_pred'] = SVR_cases
plt.figure(figsize=(16,9))
sns.relplot(data = df_plot,x = 'year',y = 'SVR_pred',hue  = 'label',kind="line")
plt.title('SVR Regression Result of San Juan')

SVR_pred_all = regressor.predict(xnor_sj)
SVR_pred_all

df_plot = pd.DataFrame({'total_cases':df_sj['total_cases'],'label':['Original'] * len(df_sj)}, index = df_sj.index)
df_plot_pred = pd.DataFrame({'total_cases':SVR_pred_all,'label':['pred']* len(df_sj)},index = df_sj.index)
df_plot_all = pd.concat([df_plot, df_plot_pred])
df_plot_all

plt.figure(figsize=(20,9))
sns.relplot(data = df_plot_all,x = df_plot_all.index,y = 'total_cases',hue  = 'label',kind="line")
plt.title('SVR Regression Result of San Juan')

regressor = SVR(kernel = 'rbf')
regressor.fit(xtrain_iq, y_train_iq)
SVR_pred = regressor.predict(xtest_iq)
r2_SVR = r2_score(y_test_iq,SVR_pred)
MSE = mean_squared_error(y_test_iq, SVR_pred)
MAE_l = mean_absolute_error(y_test_iq, SVR_pred)
print("MAE :", MAE_l)  
print("MSE :", MSE)
print("R2_Score :",r2_SVR)

SVR_pred_all = regressor.predict(xnor_iq)
df_plot = pd.DataFrame({'total_cases':df_iq['total_cases'],'label':['Original'] * len(df_iq)}, index = df_iq.index)
df_plot_pred = pd.DataFrame({'total_cases':SVR_pred_all,'label':['pred']* len(df_iq)},index = df_iq.index)
df_plot_all = pd.concat([df_plot, df_plot_pred])
df_plot_all

plt.figure(figsize=(16,9))
sns.relplot(data = df_plot_all,x = df_plot_all.index,y = 'total_cases',hue  = 'label',kind="line")
plt.title('SVR Regression Result of Iqutios')

SVR_cases = list(y_iq) + list(SVR_pred)
df_plot_iq['SVR_pred'] = SVR_cases
plt.figure(figsize=(16,9))
sns.relplot(data = df_plot_iq,x = 'year',y = 'SVR_pred',hue  = 'label',kind="line")
plt.title('SVR Regression Result of Iquitos')

"""I found that for differet city the changing trends of various temp data and ndvi data are different. In this case, we may use those data to classity different those two cities."""



"""# XGBoost """

from numpy import loadtxt
from xgboost import XGBClassifier
# fit model no training data
model = XGBClassifier()
model.fit(xtrain_sj, y_train_sj)
# make predictions for test data
y_pred = model.predict(xtest_sj)
r2_SVR = r2_score(y_test_sj,y_pred)
MSE = mean_squared_error(y_test_sj, y_pred)
MAE_l = mean_absolute_error(y_test_sj, y_pred)
print("MAE :", MAE_l)  
print("MSE :", MSE)
print("R2_Score :",r2_SVR)

from numpy import loadtxt
from xgboost import XGBClassifier
# fit model no training data
model = XGBClassifier()
model.fit(Xtrain_iq, y_train_iq)
# make predictions for test data
y_pred = model.predict(Xtest_iq)
r2_SVR = r2_score(y_test_iq,y_pred)
MSE = mean_squared_error(y_test_iq, y_pred)
MAE_l = mean_absolute_error(y_test_iq, y_pred)
print("MAE :", MAE_l)  
print("MSE :", MSE)
print("R2_Score :",r2_SVR)

from sklearn.ensemble import GradientBoostingRegressor
est = GradientBoostingRegressor(n_estimators=100, max_depth=1, random_state=0, loss='ls').fit(Xtrain_sj, y_train_sj)
yred = est.predict(Xtest_sj)
r2_SVR = r2_score(y_test_sj,yred)
MSE = mean_squared_error(y_test_sj, yred)
MAE_l = mean_absolute_error(y_test_sj, yred)
print("MAE :", MAE_l)  
print("MSE :", MSE)
print("R2_Score :",r2_SVR)

from sklearn.ensemble import GradientBoostingRegressor
est = GradientBoostingRegressor(n_estimators=100, max_depth=1, random_state=0, loss='ls').fit(Xtrain_iq, y_train_iq)
y_pred = est.predict(Xtest_iq)
r2_SVR = r2_score(y_test_iq,y_pred)
MSE = mean_squared_error(y_test_iq, y_pred)
MAE_l = mean_absolute_error(y_test_iq, y_pred)
print("MAE :", MAE_l)  
print("MSE :", MSE)
print("R2_Score :",r2_SVR)

from sklearn.ensemble import VotingRegressor
r1 = SVR(kernel = 'rbf')
r2 = RandomForestRegressor(n_estimators=175)
er = VotingRegressor([('lr', r1), ('rf', r2)])
vr = er.fit(Xtrain_sj, y_train_sj)
y_pred = vr.predict(Xtest_sj)
r2_SVR = r2_score(y_test_sj,y_pred)
MSE = mean_squared_error(y_test_sj, y_pred)
MAE_l = mean_absolute_error(y_test_sj, y_pred)
print("MAE :", MAE_l)  
print("MSE :", MSE)
print("R2_Score :",r2_SVR)

from sklearn.ensemble import VotingRegressor
r1 = SVR(kernel = 'rbf')
r2 = RandomForestRegressor(n_estimators=175)
er = VotingRegressor([('lr', r1), ('rf', r2)])
vr = er.fit(Xtrain_iq, y_train_iq)
y_pred = vr.predict(Xtest_iq)
r2_SVR = r2_score(y_test_iq,y_pred)
MSE = mean_squared_error(y_test_iq, y_pred)
MAE_l = mean_absolute_error(y_test_iq, y_pred)
print("MAE :", MAE_l)  
print("MSE :", MSE)
print("R2_Score :",r2_SVR)

